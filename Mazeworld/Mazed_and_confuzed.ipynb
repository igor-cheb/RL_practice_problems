{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c33ed110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_maze.envs.maze_env import MazeEnvRandom30x30Plus\n",
    "\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pygame\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9ed117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 00:33:20.828 Python[39122:3011561] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/p3/4j53xc_50yv4yqz952bnybrm0000gn/T/org.python.python.savedState\n"
     ]
    }
   ],
   "source": [
    "env = MazeEnvRandom30x30Plus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7fe1b749",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [214]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m act \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m      5\u001b[0m env\u001b[38;5;241m.\u001b[39mstep(act)\n\u001b[0;32m----> 6\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m.2\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(st)\n",
      "File \u001b[0;32m~/Documents/Обучение/RL/RL_practice_problems/Mazeworld/gym-maze/gym_maze/envs/maze_env.py:106\u001b[0m, in \u001b[0;36mMazeEnv.render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m close:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaze_view\u001b[38;5;241m.\u001b[39mquit_game()\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaze_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Обучение/RL/RL_practice_problems/Mazeworld/gym-maze/gym_maze/envs/maze_view_2d.py:74\u001b[0m, in \u001b[0;36mMazeView2D.update\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m         img_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__view_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__controller_update()\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/Обучение/RL/RL_practice_problems/Mazeworld/gym-maze/gym_maze/envs/maze_view_2d.py:138\u001b[0m, in \u001b[0;36mMazeView2D.__view_update\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    136\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mflipud(np\u001b[38;5;241m.\u001b[39mrot90(\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msurfarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray3d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpygame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_surface\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/RL_practice_problems-GPFC0QeZ/lib/python3.9/site-packages/pygame/surfarray.py:192\u001b[0m, in \u001b[0;36marray3d\u001b[0;34m(surface)\u001b[0m\n\u001b[1;32m    190\u001b[0m width, height \u001b[38;5;241m=\u001b[39m surface\u001b[38;5;241m.\u001b[39mget_size()\n\u001b[1;32m    191\u001b[0m array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mempty((width, height, \u001b[38;5;241m3\u001b[39m), numpy\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m--> 192\u001b[0m \u001b[43msurface_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msurface\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test code\n",
    "env.reset()\n",
    "for st in range(100):\n",
    "    act = env.action_space.sample()\n",
    "    env.step(act)\n",
    "    env.render()\n",
    "    time.sleep(.2)\n",
    "    print(st)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "551e7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_val_action_for_state(state_action_values, state):\n",
    "    res_d = {}\n",
    "    for k in state_action_values:\n",
    "        if k[0] == state:\n",
    "            res_d[k] = state_action_values[k]['value']\n",
    "    if res_d:\n",
    "        return max(res_d, key=res_d.get)[1]\n",
    "    else:\n",
    "        return env.action_space.sample()\n",
    "\n",
    "\n",
    "def choose_greedy_action(best_action, eps):\n",
    "    if np.random.uniform() < eps:\n",
    "        next_action = best_action\n",
    "    else:\n",
    "        next_action = env.action_space.sample()\n",
    "\n",
    "    return next_action\n",
    "\n",
    "\n",
    "def update_state_action_values(state_action_values, \n",
    "                               cur_state, \n",
    "                               cur_action, \n",
    "                               reward, \n",
    "                               next_state, \n",
    "                               best_action,\n",
    "                               verbose):\n",
    "\n",
    "    local_cp = state_action_values.copy()\n",
    "\n",
    "\n",
    "    if (cur_state, cur_action) in local_cp:\n",
    "        val = local_cp[(cur_state, cur_action)]['value']\n",
    "    else:\n",
    "        local_cp[(cur_state, cur_action)] = {}\n",
    "        val = 0\n",
    "        local_cp[(cur_state, cur_action)]['value'] = val\n",
    "\n",
    "    if (next_state, best_action) not in local_cp:\n",
    "        local_cp[(next_state, best_action)] = {}\n",
    "        local_cp[(next_state, best_action)]['value'] = 0\n",
    "    if verbose:\n",
    "        print('%'*30)\n",
    "        print('UPDATING Q_VALUE')\n",
    "        print('current val:', val)\n",
    "        print('TD error', reward+lam*local_cp[(next_state, best_action)]['value'] - val)\n",
    "        print('%'*30)\n",
    "    local_cp[(cur_state, cur_action)]['value'] = \\\n",
    "        val+alpha*(reward+lam*local_cp[(next_state, best_action)]['value'] - val)\n",
    "\n",
    "\n",
    "    return local_cp\n",
    "\n",
    "\n",
    "def find_all_predecessors(world_map, state):\n",
    "    res_lst = []\n",
    "    for k in world_map:\n",
    "        if world_map[k][0] == state:\n",
    "            res_lst.append(k)\n",
    "    return res_lst\n",
    "\n",
    "\n",
    "def calc_val_change(cur_state, \n",
    "                    cur_action, \n",
    "                    reward, \n",
    "                    next_state, \n",
    "                    best_action, \n",
    "                    state_action_values,\n",
    "                    verbose):\n",
    "    local_cp = state_action_values.copy()\n",
    "\n",
    "\n",
    "    if (cur_state, cur_action) in local_cp:\n",
    "        val = local_cp[(cur_state, cur_action)]['value']\n",
    "#         print(cur_state, cur_action, val)\n",
    "    else:\n",
    "        local_cp[(cur_state, cur_action)] = {}\n",
    "        val = 0\n",
    "        local_cp[(cur_state, cur_action)]['value'] = val\n",
    "\n",
    "    if (next_state, best_action) not in local_cp:\n",
    "        local_cp[(next_state, best_action)] = {}\n",
    "        local_cp[(next_state, best_action)]['value'] = 0\n",
    "\n",
    "    if verbose:\n",
    "        print('^'*30)\n",
    "        print('CURRENT:', (cur_state, cur_action), val)\n",
    "        print('reward:', reward)\n",
    "        print('lam:', lam)\n",
    "        print('Value with best act:', local_cp[(next_state, best_action)]['value'])\n",
    "        print('Res dif:', abs(reward+lam*local_cp[(next_state, best_action)]['value'] - val))\n",
    "        print('^'*30)\n",
    "\n",
    "    return abs(reward+lam*local_cp[(next_state, best_action)]['value'] - val)\n",
    "\n",
    "# [cur_state, action, val_change]\n",
    "\n",
    "\n",
    "def optimise_world(world_map, state_action_values, PQueue, change_thresh, verbose):\n",
    "\n",
    "    local_state_action_values = state_action_values.copy()\n",
    "    local_world_map = world_map.copy()\n",
    "\n",
    "#     priority_move = list(np.array(PQueue, dtype=object)[np.array(PQueue, dtype=object)[:, 2].argsort()[::-1]][0])\n",
    "\n",
    "    if verbose: \n",
    "        print('-'*30)\n",
    "        print('PQueue:', PQueue)\n",
    "        print('Best move:', priority_move)\n",
    "\n",
    "#     PQueue.pop(PQueue.index(priority_move))\n",
    "\n",
    "    (cur_state, cur_action) = random.choice(list(local_world_map.keys())) #(priority_move[0], priority_move[1]) #\n",
    "    (next_state, reward) = local_world_map[(cur_state, cur_action)]\n",
    "    best_action = find_max_val_action_for_state(local_state_action_values, next_state)\n",
    "    if verbose: print('BEST ACT:', best_action)\n",
    "    if verbose: print(f'VAL BEFORE {(cur_state, cur_action)}', local_state_action_values[(cur_state, cur_action)])\n",
    "    local_state_action_values = update_state_action_values(local_state_action_values, \n",
    "                                                           cur_state, \n",
    "                                                           cur_action, \n",
    "                                                           reward, \n",
    "                                                           next_state, \n",
    "                                                           best_action, \n",
    "                                                           verbose=False)\n",
    "    if verbose: print(f'VAL AFTER {(cur_state, cur_action)}', local_state_action_values[(cur_state, cur_action)])\n",
    "\n",
    "#     all_preds = find_all_predecessors(local_world_map, cur_state)\n",
    "#     if verbose: print('STATE', cur_state, 'ALL PREDS', all_preds)\n",
    "#     for pred in all_preds:\n",
    "#         (pred_cur_state, pred_action) = (pred[0], pred[1])\n",
    "#         (pred_next_state, pred_rew) = local_world_map[pred]\n",
    "#         pred_best_action = find_max_val_action_for_state(local_state_action_values, pred_next_state)\n",
    "#         pred_val_change = calc_val_change(pred_cur_state, \n",
    "#                                           pred_action, \n",
    "#                                           pred_rew, \n",
    "#                                           pred_next_state, \n",
    "#                                           pred_best_action,\n",
    "#                                           local_state_action_values,\n",
    "#                                           verbose=False)\n",
    "#         if verbose: print(pred, pred_val_change)\n",
    "#         if pred_val_change > change_thresh:\n",
    "#             tmp_q = [k for k in PQueue if k[0] == pred_cur_state and k[1] == pred_action]\n",
    "#             if verbose: print('INNER TMP_Q:', tmp_q)\n",
    "#             if not tmp_q:\n",
    "#                 PQueue.append([pred_cur_state, pred_action, pred_val_change])\n",
    "    return (local_state_action_values, PQueue)\n",
    "\n",
    "\n",
    "def run_episode(env, cur_state, cur_action, policy, state_action_values, show=False):\n",
    "    done = False\n",
    "    trajectory = []\n",
    "    while not done:\n",
    "        next_state, reward, done, info = env.step(cur_action); next_state = tuple(next_state)\n",
    "        if next_state in policy:\n",
    "            next_action = policy[next_state]\n",
    "        else:\n",
    "            best_action = find_max_val_action_for_state(state_action_values, next_state)\n",
    "            next_action =  choose_greedy_action(best_action, eps)\n",
    "\n",
    "#             next_action = select_greedy_action(next_state, state_action_values, eps)\n",
    "\n",
    "        # Now we have: (cur_state, cur_action, reward, new_state, next_action)\n",
    "        trajectory.append((cur_state, cur_action, reward, next_state, next_action))\n",
    "        print(len(trajectory))\n",
    "        cur_state = next_state; cur_action = next_action\n",
    "        if show:\n",
    "            env.render()\n",
    "            time.sleep(.2)\n",
    "            clear_output(wait=True)\n",
    "\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "def learn_episode(env, \n",
    "                  cur_state, \n",
    "                  cur_action, \n",
    "                  state_action_values, \n",
    "                  policy,\n",
    "                  world_map,\n",
    "                  ep_num,\n",
    "                  PQueue,\n",
    "                  change_thresh=0.000005,\n",
    "                  verbose=True,\n",
    "                  show=False):\n",
    "    local_state_action_values = state_action_values.copy()\n",
    "    local_policy = policy.copy()\n",
    "    local_world_map = world_map.copy()\n",
    "    ep_sum_reward = 0\n",
    "    trajectory=[]\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        next_state, reward, done, info = env.step(cur_action); ep_sum_reward+=reward\n",
    "        next_state = tuple(next_state)\n",
    "        local_world_map[(cur_state, cur_action)] = (next_state, reward)\n",
    "\n",
    "        best_action = find_max_val_action_for_state(local_state_action_values, next_state)\n",
    "        next_action = choose_greedy_action(best_action, eps)\n",
    "        val_change = calc_val_change(cur_state, cur_action, reward, next_state, \n",
    "                                     best_action, state_action_values, verbose=False)\n",
    "        if val_change > change_thresh:\n",
    "            tmp_q = [k for k in PQueue if k[0] == cur_state and k[1] == cur_action]\n",
    "            if not tmp_q:\n",
    "                if verbose: print('ADDED IN OUTTER:', [cur_state, cur_action, val_change])\n",
    "                PQueue.append([cur_state, cur_action, val_change])\n",
    "\n",
    "        local_policy[next_state] = best_action\n",
    "        trajectory.append((cur_state, cur_action, reward, next_state))\n",
    "\n",
    "        local_state_action_values = \\\n",
    "            update_state_action_values(local_state_action_values, cur_state, cur_action, reward, \n",
    "                                       next_state, best_action, verbose=False)\n",
    "\n",
    "        if (ep_num > 0):\n",
    "            st_num = 50 #round(2*np.sqrt(len(local_world_map)))\n",
    "            for model_step in range(st_num): #2*len(local_world_map)\n",
    "#                 if PQueue:\n",
    "                if verbose: print('PQueue len:', len(PQueue))\n",
    "                (local_state_action_values, PQueue) = optimise_world(local_world_map, \n",
    "                                                                     local_state_action_values, \n",
    "                                                                     PQueue, \n",
    "                                                                     change_thresh,\n",
    "                                                                     verbose)\n",
    "                    \n",
    "        cur_state = next_state; cur_action = next_action\n",
    "\n",
    "        if show:\n",
    "            env.render()\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(.3)\n",
    "\n",
    "    return (trajectory, \n",
    "            local_state_action_values, \n",
    "            local_policy, \n",
    "            ep_sum_reward, \n",
    "            local_world_map, \n",
    "            PQueue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fdf8ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EP NUM 0 START\n",
      "EP NUM 0 END: -2.373333333331711 30361\n",
      "##############################\n",
      "EP NUM 1 START\n",
      "EP NUM 1 END: 0.2557777777778617 6699\n",
      "##############################\n",
      "EP NUM 2 START\n",
      "EP NUM 2 END: 0.9908888888888889 83\n",
      "##############################\n",
      "EP NUM 3 START\n",
      "EP NUM 3 END: 0.9891111111111112 99\n",
      "##############################\n",
      "EP NUM 4 START\n",
      "EP NUM 4 END: 0.9787777777777779 192\n",
      "##############################\n",
      "EP NUM 5 START\n",
      "EP NUM 5 END: 0.9812222222222223 170\n",
      "##############################\n",
      "EP NUM 6 START\n",
      "EP NUM 6 END: 0.9903333333333333 88\n",
      "##############################\n",
      "EP NUM 7 START\n",
      "EP NUM 7 END: 0.9887777777777778 102\n",
      "##############################\n",
      "EP NUM 8 START\n",
      "EP NUM 8 END: 0.9841111111111112 144\n",
      "##############################\n",
      "EP NUM 9 START\n",
      "EP NUM 9 END: 0.9887777777777778 102\n",
      "##############################\n",
      "EP NUM 10 START\n",
      "EP NUM 10 END: 0.9884444444444445 105\n",
      "##############################\n",
      "EP NUM 11 START\n",
      "EP NUM 11 END: 0.9912222222222222 80\n",
      "##############################\n",
      "EP NUM 12 START\n",
      "EP NUM 12 END: 0.9898888888888889 92\n",
      "##############################\n",
      "EP NUM 13 START\n",
      "EP NUM 13 END: 0.9763333333333334 214\n",
      "##############################\n",
      "EP NUM 14 START\n",
      "EP NUM 14 END: 0.9826666666666667 157\n",
      "##############################\n",
      "EP NUM 15 START\n",
      "EP NUM 15 END: 0.9592222222222224 368\n",
      "##############################\n",
      "EP NUM 16 START\n",
      "EP NUM 16 END: 0.9820000000000001 163\n",
      "##############################\n",
      "EP NUM 17 START\n",
      "EP NUM 17 END: 0.9862222222222222 125\n",
      "##############################\n",
      "EP NUM 18 START\n",
      "EP NUM 18 END: 0.9914444444444445 78\n",
      "##############################\n",
      "EP NUM 19 START\n",
      "EP NUM 19 END: 0.9864444444444445 123\n",
      "##############################\n",
      "EP NUM 20 START\n",
      "EP NUM 20 END: 0.9922222222222222 71\n",
      "##############################\n",
      "EP NUM 21 START\n",
      "EP NUM 21 END: 0.987 118\n",
      "##############################\n",
      "EP NUM 22 START\n",
      "EP NUM 22 END: 0.9901111111111112 90\n",
      "##############################\n",
      "EP NUM 23 START\n",
      "EP NUM 23 END: 0.9905555555555555 86\n",
      "##############################\n",
      "EP NUM 24 START\n",
      "EP NUM 24 END: 0.9907777777777778 84\n",
      "##############################\n",
      "EP NUM 25 START\n",
      "EP NUM 25 END: 0.9906666666666667 85\n",
      "##############################\n",
      "EP NUM 26 START\n",
      "EP NUM 26 END: 0.9903333333333333 88\n",
      "##############################\n",
      "EP NUM 27 START\n",
      "EP NUM 27 END: 0.9892222222222222 98\n",
      "##############################\n",
      "EP NUM 28 START\n",
      "EP NUM 28 END: 0.9893333333333333 97\n",
      "##############################\n",
      "EP NUM 29 START\n",
      "EP NUM 29 END: 0.9897777777777778 93\n",
      "##############################\n",
      "EP NUM 30 START\n",
      "EP NUM 30 END: 0.9905555555555555 86\n",
      "##############################\n",
      "EP NUM 31 START\n",
      "EP NUM 31 END: 0.9892222222222222 98\n",
      "##############################\n",
      "EP NUM 32 START\n",
      "EP NUM 32 END: 0.9901111111111112 90\n",
      "##############################\n",
      "EP NUM 33 START\n",
      "EP NUM 33 END: 0.9868888888888889 119\n",
      "##############################\n",
      "EP NUM 34 START\n",
      "EP NUM 34 END: 0.9913333333333333 79\n",
      "##############################\n",
      "EP NUM 35 START\n",
      "EP NUM 35 END: 0.9917777777777778 75\n",
      "##############################\n",
      "EP NUM 36 START\n",
      "EP NUM 36 END: 0.9896666666666667 94\n",
      "##############################\n",
      "EP NUM 37 START\n",
      "EP NUM 37 END: 0.9912222222222222 80\n",
      "##############################\n",
      "EP NUM 38 START\n",
      "EP NUM 38 END: 0.9906666666666667 85\n",
      "##############################\n",
      "EP NUM 39 START\n",
      "EP NUM 39 END: 0.9894444444444445 96\n",
      "##############################\n",
      "EP NUM 40 START\n",
      "EP NUM 40 END: 0.9914444444444445 78\n",
      "##############################\n",
      "EP NUM 41 START\n",
      "EP NUM 41 END: 0.9928888888888889 65\n",
      "##############################\n",
      "EP NUM 42 START\n",
      "EP NUM 42 END: 0.9915555555555555 77\n",
      "##############################\n",
      "EP NUM 43 START\n",
      "EP NUM 43 END: 0.991 82\n",
      "##############################\n",
      "EP NUM 44 START\n",
      "EP NUM 44 END: 0.9882222222222222 107\n",
      "##############################\n",
      "EP NUM 45 START\n",
      "EP NUM 45 END: 0.9897777777777778 93\n",
      "##############################\n",
      "EP NUM 46 START\n",
      "EP NUM 46 END: 0.9914444444444445 78\n",
      "##############################\n",
      "EP NUM 47 START\n",
      "EP NUM 47 END: 0.9905555555555555 86\n",
      "##############################\n",
      "EP NUM 48 START\n",
      "EP NUM 48 END: 0.9886666666666667 103\n",
      "##############################\n",
      "EP NUM 49 START\n",
      "EP NUM 49 END: 0.991111111111111 81\n",
      "##############################\n"
     ]
    }
   ],
   "source": [
    "eps = 0.6\n",
    "alpha = 0.5\n",
    "lam = 0.8\n",
    "\n",
    "world_map = {}\n",
    "state_action_values = {}\n",
    "policy = {}\n",
    "PQueue = []\n",
    "\n",
    "episodes = 50\n",
    "ep_sums = []\n",
    "trajectories = []\n",
    "for ep_num in range(episodes):\n",
    "    cur_state = tuple(env.reset())\n",
    "    cur_action =  env.action_space.sample()\n",
    "    verbose = True if (ep_num > 0) else False\n",
    "    print(f'EP NUM {ep_num} START')\n",
    "    (trajectory, state_action_values, policy, ep_sum_reward, world_map, PQueue) = \\\n",
    "        learn_episode(env,\n",
    "                      cur_state,\n",
    "                      cur_action,\n",
    "                      state_action_values,\n",
    "                      policy,\n",
    "                      world_map,\n",
    "                      ep_num,\n",
    "                      PQueue,\n",
    "                      verbose=False,\n",
    "                      show=False)\n",
    "    trajectories.append(trajectory)\n",
    "    ep_sums.append(ep_sum_reward)\n",
    "\n",
    "    if ep_num%1==0:\n",
    "        print(f'EP NUM {ep_num} END:', ep_sum_reward, len(trajectory))\n",
    "        print('#'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8404cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = find_all_predecessors(local_world_map, cur_state)\n",
    "\n",
    "for pred in all_preds:\n",
    "    (pred_cur_state, pred_action) = (pred[0], pred[1])\n",
    "    (pred_next_state, pred_rew) = local_world_map[pred]\n",
    "    pred_best_action = find_max_val_action_for_state(local_state_action_values, pred_next_state)\n",
    "    pred_val_change = calc_val_change(pred_cur_state, \n",
    "                                      pred_action, \n",
    "                                      pred_rew, \n",
    "                                      pred_next_state, \n",
    "                                      pred_best_action,\n",
    "                                      local_state_action_values,\n",
    "                                      verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3651ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 500.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PLOT TRAJECTORIES LENGTH\n",
    "\n",
    "\n",
    "lens = []\n",
    "for t in trajectories:#[:1500]:\n",
    "    lens.append(len(t))\n",
    "lens_df = pd.DataFrame(lens)\n",
    "lens_df['mean_len'] = lens_df[0].rolling(5).mean()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(lens_df['mean_len'])\n",
    "plt.ylim(0,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38042628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m cur_state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      6\u001b[0m cur_action \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m----> 7\u001b[0m new_episode \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcur_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcur_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mstate_action_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSUCCESSS!!! \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(env, cur_state, cur_action, policy, state_action_values, show)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m show:\n\u001b[1;32m    167\u001b[0m         env\u001b[38;5;241m.\u001b[39mrender()\n\u001b[0;32m--> 168\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m         clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trajectory\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# POLICY DEMO\n",
    "num_eps = 500\n",
    "\n",
    "for eps in range(num_eps):\n",
    "    cur_state = env.reset()\n",
    "    cur_action = env.action_space.sample()\n",
    "    new_episode = run_episode(env, \n",
    "                              cur_state, \n",
    "                              cur_action, \n",
    "                              policy, \n",
    "                              state_action_values, \n",
    "                              show=True)\n",
    "    print('SUCCESSS!!! '*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d41ced4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 2,\n",
       " (1, 0): 0,\n",
       " (1, 1): 0,\n",
       " (0, 1): 1,\n",
       " (0, 2): 1,\n",
       " (1, 2): 3,\n",
       " (0, 3): 1,\n",
       " (0, 4): 1,\n",
       " (0, 5): 1,\n",
       " (0, 6): 2,\n",
       " (1, 6): 2,\n",
       " (2, 6): 3,\n",
       " (3, 6): 1,\n",
       " (2, 2): 3,\n",
       " (2, 1): 1,\n",
       " (3, 1): 0,\n",
       " (3, 0): 3,\n",
       " (2, 0): 3,\n",
       " (3, 7): 1,\n",
       " (3, 8): 2,\n",
       " (4, 8): 2,\n",
       " (5, 8): 0,\n",
       " (5, 7): 3,\n",
       " (4, 7): 0,\n",
       " (4, 6): 0,\n",
       " (4, 5): 3,\n",
       " (3, 5): 0,\n",
       " (3, 4): 3,\n",
       " (2, 4): 0,\n",
       " (2, 3): 2,\n",
       " (3, 3): 2,\n",
       " (4, 3): 2,\n",
       " (5, 3): 0,\n",
       " (5, 2): 2,\n",
       " (6, 2): 2,\n",
       " (7, 2): 0,\n",
       " (7, 1): 3,\n",
       " (6, 1): 3,\n",
       " (5, 1): 3,\n",
       " (4, 1): 0,\n",
       " (4, 0): 2,\n",
       " (5, 0): 2,\n",
       " (6, 0): 2,\n",
       " (7, 0): 2,\n",
       " (1, 3): 2,\n",
       " (1, 4): 0,\n",
       " (1, 5): 0,\n",
       " (2, 5): 3,\n",
       " (4, 2): 0,\n",
       " (3, 2): 2,\n",
       " (8, 0): 1,\n",
       " (8, 1): 2,\n",
       " (9, 1): 1,\n",
       " (9, 2): 3,\n",
       " (8, 2): 1,\n",
       " (9, 0): 1,\n",
       " (8, 3): 2,\n",
       " (9, 3): 1,\n",
       " (9, 4): 3,\n",
       " (8, 4): 3,\n",
       " (7, 4): 3,\n",
       " (6, 4): 1,\n",
       " (6, 3): 1,\n",
       " (7, 3): 3,\n",
       " (6, 5): 1,\n",
       " (6, 6): 1,\n",
       " (5, 6): 2,\n",
       " (6, 7): 1,\n",
       " (6, 8): 2,\n",
       " (7, 8): 0,\n",
       " (7, 7): 0,\n",
       " (7, 6): 2,\n",
       " (8, 6): 2,\n",
       " (9, 6): 1,\n",
       " (9, 7): 3,\n",
       " (8, 7): 1,\n",
       " (8, 8): 1,\n",
       " (8, 9): 2,\n",
       " (7, 9): 2,\n",
       " (6, 9): 2,\n",
       " (5, 9): 1,\n",
       " (4, 9): 2,\n",
       " (3, 9): 2,\n",
       " (2, 9): 2,\n",
       " (2, 8): 1,\n",
       " (2, 7): 1,\n",
       " (1, 7): 2,\n",
       " (1, 8): 0,\n",
       " (1, 9): 0,\n",
       " (0, 9): 3,\n",
       " (0, 8): 3,\n",
       " (0, 7): 1,\n",
       " (9, 5): 1,\n",
       " (8, 5): 0,\n",
       " (7, 5): 2,\n",
       " (5, 5): 1,\n",
       " (5, 4): 1,\n",
       " (4, 4): 2,\n",
       " (9, 9): 2}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98f6e0",
   "metadata": {},
   "source": [
    "# Prioritised sweeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33541c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_mazeworld",
   "language": "python",
   "name": "for_mazeworld"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
